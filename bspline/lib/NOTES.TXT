
To find "optimum" number of nodes, we want to maximize number
of data points per node interval (which means maximizing the 
interval size), while maximizing the number of nodes per 
wavelength, which means minimizing the interval size.  

Need to add partial quadratures to calculation of Q at boundaries.

Calculation of P should only loop over the basis functions
nearby each point X[J] which will have a contribution to the matrix
entry, then take advantage of symmetry to avoid redundant
calculations.

The Ooyama fortran code first removes the mean from the y values before
smoothing.  We could do the same, keeping the mean in the object state,
and adding it back in when evaluating.

When calculating mean, we can test for order in the X values and use it
to help limit the range of X values which need to be calculated for the
b vector (see below).

In addP(), try the faster algorithm currently ifdef'd out, but add DX
to the calculation, since that seems to be what was missing in the
slow version.

Move DX out of addP() loop over sums, and multiply it once at the left edge
of the box currently being calculated, since those sums will be done at the end
of the current box.

Any way to limit the summations in calculating the b vector to the X data
points which will actually produce non-zero values in the basis function?
Is it even worth it?

We should be able to use the "recursive" solution from the Ooyama paper by
first performing Gaussian elimination to conver the P+Q matrix to an upper
diagonal matrix.  Pivoting should be unnecessary, since the diagonal should
have the larger values and always be non-zero.  The diagonalized matrix can
be used to solve against each B vector using the recursive method.  In fact, since
the E vectors do not depend upon the B vector, only the F vector, we can compute
E ahead of time also.  And only the diagonals of P+Q should be stored, rather than
the full square matrix.

The BSplineSolver can either keep a reference to M and use it back-solve for a 
given b, else it can forget M after it has generated the e and f vectors (keeping
the M elements needed for f in f, possibly as their inverse).  Then a Solver would
not need any further reference to M and can still solve for given b vectors, but
ultimately it requires more multiplications than using M directly.  (Access to M
may be slow enough to warrant using the private vectors instead.)

The problem with solver keeping a reference to Q, when it is copied 
in the construction of a BSpline subclass, it will still
refer to the Q in the BSplineBase class, rather than the new copy of BSplineBaseP
in the subclass object.  At least using a default constructor in baseP set the
reference to the new Q without calling upper().

Bug: Setup() probably in infinite loop when not enough data points for a smaller
cutoff wavelength.

Deriving the least squares between the actual and approximated u(x) results in
a set of linear equations for A independent of DX.  This makes sense, since we could
have divided both sides of the (P+Q)a = b equation by DX without affecting the solution.
Oops, we still need to divide Q, or at least not include DX in the elements of Q.  Oops
again, it seems to work without dividing Q, though I don't know what the implications are
for the stability of the linear system.  Mathematically it makes sense since the least
squares is just the addition of the derivative constraint, which can be minimized 
separately.

The algorithm seems to be working well for the wind speed data I have.  I don't know
what Charlie's problem is.  Two approaches:

	Copy his routine to generate test curve and use it as source data.

	Try building spline on unix and linking it with FORTRAN and compare the results,
	especially for Charlie's tests.

Need to add kth derivatives 2 and 3, since FORTRAN code may use the higher derivatives
and not the first as is coded now.



