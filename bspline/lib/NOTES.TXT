
To find "optimum" number of nodes, we want to maximize number
of data points per node interval (which means maximizing the 
interval size), while maximizing the number of nodes per 
wavelength, which means minimizing the interval size.  

Need to add partial quadratures to calculation of Q at boundaries.

Calculation of P should only loop over the basis functions
nearby each point X[J] which will have a contribution to the matrix
entry, then take advantage of symmetry to avoid redundant
calculations.

The Ooyama fortran code first removes the mean from the y values before
smoothing.  We could do the same, keeping the mean in the object state,
and adding it back in when evaluating.

When calculating mean, we can test for order in the X values and use it
to help limit the range of X values which need to be calculated for the
b vector (see below).

In addP(), try the faster algorithm currently ifdef'd out, but add DX
to the calculation, since that seems to be what was missing in the
slow version.

Move DX out of addP() loop over sums, and multiply it once at the left edge
of the box currently being calculated, since those sums will be done at the end
of the current box.

Any way to limit the summations in calculating the b vector to the X data
points which will actually produce non-zero values in the basis function?
Is it even worth it?

We should be able to use the "recursive" solution from the Ooyama paper by
first performing Gaussian elimination to conver the P+Q matrix to an upper
diagonal matrix.  Pivoting should be unnecessary, since the diagonal should
have the larger values and always be non-zero.  The diagonalized matrix can
be used to solve against each B vector using the recursive method.  In fact, since
the E vectors do not depend upon the B vector, only the F vector, we can compute
E ahead of time also.  And only the diagonals of P+Q should be stored, rather than
the full square matrix.

Bug: Setup() probably in infinite loop when not enough data points for a smaller
cutoff wavelength.


