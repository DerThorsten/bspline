
To find "optimum" number of nodes, we want to maximize number
of data points per node interval (which means maximizing the 
interval size), while maximizing the number of nodes per 
wavelength, which means minimizing the interval size.  

Calculation of P should only loop over the basis functions
nearby each point X[J] which will have a contribution to the matrix
entry, then take advantage of symmetry to avoid redundant
calculations.

When calculating mean, we can test for order in the X values and use it
to help limit the range of X values which need to be calculated for the
b vector (see below).

Any way to limit the summations in calculating the b vector to the X data
points which will actually produce non-zero values in the basis function?
Is it even worth it?

We should be able to use the "recursive" solution from the Ooyama paper by
first performing Gaussian elimination to conver the P+Q matrix to an upper
diagonal matrix.  Pivoting should be unnecessary, since the diagonal should
have the larger values and always be non-zero.  The diagonalized matrix can
be used to solve against each B vector using the recursive method.  In
fact, since the E vectors do not depend upon the B vector, only the F
vector, we can compute E ahead of time also.  Problem: need B vector when
performing the gaussian elimination, so use LU factoring limited to the
diagonal bands.

Only the diagonals of P+Q should be stored, rather than
the full square matrix: hence the BandedMatrix template class.

The BSplineSolver can either keep a reference to M and use it back-solve
for a given b, else it can forget M after it has generated the e and f
vectors (keeping the M elements needed for f in f, possibly as their
inverse).  Then a Solver would not need any further reference to M and can
still solve for given b vectors, but ultimately it requires more
multiplications than using M directly.  (Access to M may be slow enough to
warrant using the private vectors instead.)

The problem with solver keeping a reference to Q, when it is copied in the
construction of a BSpline subclass, it will still refer to the Q in the
BSplineBase class, rather than the new copy of BSplineBaseP in the subclass
object.  At least using a default constructor in baseP set the reference to
the new Q without calling upper().

Deriving the least squares between the actual and approximated u(x) results
in a set of linear equations for A independent of DX.  This makes sense,
since we could have divided both sides of the (P+Q)a = b equation by DX
without affecting the solution.  Oops, we still need to divide Q, or at
least not include DX in the elements of Q.  Oops again, it seems to work
without dividing Q, though I don't know what the implications are for the
stability of the linear system.  Mathematically it makes sense since the
least squares is just the addition of the derivative constraint, which can
be minimized (i.e., set to zero) separately.

The algorithm seems to be working well for the wind speed data I have.  I
can't figure out Charlie's problem.  Two approaches:

	Copy his routine to generate test curve and use it as source data.

	Try building spline on unix and linking it with FORTRAN and compare
	the results, especially for Charlie's tests.

Did the latter and discovered the alpha discrepancy described below.
Eventually got reasonable agreement with fortran, except over significant
data gaps.  Charlie's problem turned out to be with plot scaling.

Do a binary search to find the X nearest a node, then add up the X's before
and after it which would have nonzero basis() values.  Basis() appears to
dominate the profile timing because of the number of times it is called, so
if we can reduce the number of calls, that should really help the speed.

We're getting 524568 calls to Basis with parameters 2 15 < wdir.  Refining
addP() more closely brings it down to 132075.  Much better.  Looping over X
to sum B elements results in 19024 calls.  Even better.  I don't think
sorting will give us any benefits; forget about it.  Refining range of
Basis calls in evaluate gets us to 17781.  That looks like the smallest
I'll get it.  Besides the fortran call to invmtx, the i/o is dominating the
process time.

It looks like the fortran code calculates alpha as 

	Z2 = YSDCPI**K, where YSDCPI = YDCWL/(PI2*DYN)

In other words, if alpha is Z2, alpha includes deltax.

FORTRAN code calculates the QNRBG array first.  It might be useful for
BSpline to do the same, with each object having its own copy of the beta
parameters and Q quadratures in place, depending upon the choice for
boundary condition types, since they need to be computed for types other
than 0, 1, and 2 which is all that's supported now.

It would be interesting to feed the data before and after smoothing through
a fourier transform to see the effects of the derivative constraint.

Remove the index vector for the LU factor() and solve() routines and from
the bspline base.
 --> Done.

Add a BSpline constructor which takes the same arguments as a Base,
constructs the base, then performs the usual spline construction.  Avoids
copying the base, and it can still be used to apply() to new curves.
 --> Done.

How would we apply new curves with perhaps missing y values at some of the
nodes?  We could always recalculate the base and return a spline with a
different base, if necessary.  Else we could interpolate in simple cases?
Just set it to zero?  Could we break up the curve into segments, and smooth
them separately, perhaps with boundary conditions at the endpoints to
account for the missing value?

To be rigorous, I need to be able to prove that I only need to calculate
the LU factorization on the diagonal bands, and that pivoting is
unnecessary because the diagonal will always dominate.  Ooyama casually
mentions this in the paper I think.

Compute and solve the B vector in place, rather than copying into the A
first.
 --> Done.

LU_solve_banded doesn't need to multiply A elements outside the diagonals.
Fix that.
 --> Done.

Might be nice to include the banded solvers in BandedMatrix instead of a
separate header file.  Also, try subclassing vector<> to add an operator()
so that the solver routines can be re-written without the [] and -1
offsets.


